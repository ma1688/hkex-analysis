#!/usr/bin/env python3
"""
æ¸…ç†é‡å¤æ•°æ®è„šæœ¬
ç”¨äºåˆ é™¤åŸºäºfile_pathçš„é‡å¤æ–‡æ¡£è®°å½•
"""

import sys
from pathlib import Path

# å¯¼å…¥é¡¹ç›®é…ç½®
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.config.settings import get_settings
import clickhouse_connect

settings = get_settings()

def cleanup_duplicates(dry_run: bool = True):
    """
    æ¸…ç†é‡å¤æ–‡æ¡£
    
    Args:
        dry_run: True=ä»…æ˜¾ç¤ºå°†è¦åˆ é™¤çš„è®°å½•ï¼ŒFalse=çœŸæ­£åˆ é™¤
    """
    client = clickhouse_connect.get_client(
        host=settings.clickhouse_host,
        port=settings.clickhouse_port,
        database=settings.clickhouse_database,
        username=settings.clickhouse_user,
        password=settings.clickhouse_password
    )
    
    print("=" * 80)
    print("  æ¸…ç†é‡å¤æ•°æ®å·¥å…·")
    print("=" * 80)
    
    # 1. æŸ¥æ‰¾é‡å¤æ–‡æ¡£ï¼ˆåŸºäºfile_pathï¼‰
    print("\nğŸ” æŸ¥æ‰¾é‡å¤æ–‡æ¡£...")
    
    duplicates_query = """
    SELECT 
        file_path,
        groupArray(doc_id) as doc_ids,
        count() as cnt
    FROM documents_v2
    GROUP BY file_path
    HAVING cnt > 1
    ORDER BY cnt DESC
    """
    
    duplicates = client.query(duplicates_query).result_rows
    
    if not duplicates:
        print("âœ… æœªå‘ç°é‡å¤æ–‡æ¡£")
        return
    
    print(f"âš ï¸  å‘ç° {len(duplicates)} ä¸ªæ–‡ä»¶æœ‰é‡å¤è®°å½•")
    print("-" * 80)
    
    total_to_delete = 0
    
    for file_path, doc_ids, count in duplicates:
        print(f"\næ–‡ä»¶: {file_path}")
        print(f"é‡å¤è®°å½•æ•°: {count}")
        print(f"doc_ids: {doc_ids}")
        
        # ä¿ç•™æœ€æ–°çš„è®°å½•ï¼Œåˆ é™¤æ—§è®°å½•
        if len(doc_ids) > 1:
            # æŸ¥è¯¢åˆ›å»ºæ—¶é—´ï¼Œä¿ç•™æœ€æ–°çš„
            time_query = f"""
            SELECT doc_id, created_at 
            FROM documents_v2 
            WHERE doc_id IN {tuple(doc_ids)}
            ORDER BY created_at DESC
            """
            
            records = client.query(time_query).result_rows
            keep_doc_id = records[0][0]  # æœ€æ–°çš„
            delete_doc_ids = [r[0] for r in records[1:]]  # è¦åˆ é™¤çš„
            
            print(f"  ä¿ç•™: {keep_doc_id}")
            print(f"  åˆ é™¤: {delete_doc_ids}")
            
            total_to_delete += len(delete_doc_ids)
            
            if not dry_run:
                # åˆ é™¤æ—§çš„æ–‡æ¡£è®°å½•
                for doc_id in delete_doc_ids:
                    # å…ˆåˆ é™¤ç« èŠ‚
                    client.command(f"ALTER TABLE document_sections DELETE WHERE doc_id = '{doc_id}'")
                    # å†åˆ é™¤æ–‡æ¡£
                    client.command(f"ALTER TABLE documents_v2 DELETE WHERE doc_id = '{doc_id}'")
                    print(f"  âœ… å·²åˆ é™¤: {doc_id}")
    
    print("\n" + "=" * 80)
    if dry_run:
        print(f"âš ï¸  é¢„æ¼”æ¨¡å¼ï¼šå°†åˆ é™¤ {total_to_delete} æ¡è®°å½•")
        print("è¿è¡Œ 'python cleanup_duplicates.py --execute' æ‰§è¡ŒçœŸæ­£åˆ é™¤")
    else:
        print(f"âœ… å·²åˆ é™¤ {total_to_delete} æ¡è®°å½•")
        print("æ­£åœ¨ä¼˜åŒ–è¡¨...")
        client.command("OPTIMIZE TABLE documents_v2 FINAL")
        client.command("OPTIMIZE TABLE document_sections FINAL")
        print("âœ… è¡¨ä¼˜åŒ–å®Œæˆ")
    print("=" * 80)
    
    client.close()


def main():
    dry_run = True
    
    if len(sys.argv) > 1 and sys.argv[1] == '--execute':
        dry_run = False
        print("âš ï¸  è­¦å‘Šï¼šå°†æ‰§è¡ŒçœŸæ­£çš„åˆ é™¤æ“ä½œï¼")
        response = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): ")
        if response.lower() != 'yes':
            print("å·²å–æ¶ˆ")
            return
    
    cleanup_duplicates(dry_run=dry_run)


if __name__ == '__main__':
    main()

